{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T21:32:18.233419Z",
     "start_time": "2021-06-08T21:32:18.039495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storage_dimm2/kaggle_data/commonlitreadabilityprize/mlm_data_val.csv\n",
      "/mnt/storage_dimm2/kaggle_data/commonlitreadabilityprize/pretrain_text.txt\n",
      "/mnt/storage_dimm2/kaggle_data/commonlitreadabilityprize/test.csv\n",
      "/mnt/storage_dimm2/kaggle_data/commonlitreadabilityprize/mlm_data.csv\n",
      "/mnt/storage_dimm2/kaggle_data/commonlitreadabilityprize/sample_submission.csv\n",
      "/mnt/storage_dimm2/kaggle_data/commonlitreadabilityprize/train.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "INPUT_PATH = Path(f\"/mnt/storage_dimm2/kaggle_data/commonlitreadabilityprize\")\n",
    "MODEL_CACHE = Path(\"/mnt/storage/model_cache/torch\")\n",
    "\n",
    "for child in INPUT_PATH.iterdir():\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T21:32:18.338779Z",
     "start_time": "2021-06-08T21:32:18.295920Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id url_legal license  \\\n",
       "0  c12129c31       NaN     NaN   \n",
       "1  85aa80a4c       NaN     NaN   \n",
       "2  b69ac6792       NaN     NaN   \n",
       "3  dd1000b26       NaN     NaN   \n",
       "4  37c1b32fb       NaN     NaN   \n",
       "\n",
       "                                             excerpt    target  standard_error  \n",
       "0  When the young people returned to the ballroom... -0.340259        0.464009  \n",
       "1  All through dinner time, Mrs. Fayre was somewh... -0.315372        0.480805  \n",
       "2  As Roger had predicted, the snow departed as q... -0.580118        0.476676  \n",
       "3  And outside before the palace a great garden w... -1.054013        0.450007  \n",
       "4  Once upon a time there were Three Bears who li...  0.247197        0.510845  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(INPUT_PATH / \"train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T21:32:18.588540Z",
     "start_time": "2021-06-08T21:32:18.571646Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2834.000000</td>\n",
       "      <td>2834.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.959319</td>\n",
       "      <td>0.491435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.033579</td>\n",
       "      <td>0.034818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.676268</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.690320</td>\n",
       "      <td>0.468543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.912190</td>\n",
       "      <td>0.484721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-0.202540</td>\n",
       "      <td>0.506268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.711390</td>\n",
       "      <td>0.649671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            target  standard_error\n",
       "count  2834.000000     2834.000000\n",
       "mean     -0.959319        0.491435\n",
       "std       1.033579        0.034818\n",
       "min      -3.676268        0.000000\n",
       "25%      -1.690320        0.468543\n",
       "50%      -0.912190        0.484721\n",
       "75%      -0.202540        0.506268\n",
       "max       1.711390        0.649671"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T21:32:19.254406Z",
     "start_time": "2021-06-08T21:32:18.797715Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"target\"].hist(bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T21:32:19.259221Z",
     "start_time": "2021-06-08T21:32:19.255659Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When the young people returned to the ballroom, it presented a decidedly changed appearance. Instead of an interior scene, it was a winter landscape.\\nThe floor was covered with snow-white canvas, not laid on smoothly, but rumpled over bumps and hillocks, like a real snow field. The numerous palms and evergreens that had decorated the room, were powdered with flour and strewn with tufts of cotton, like snow. Also diamond dust had been lightly sprinkled on them, and glittering crystal icicles hung from the branches.\\nAt each end of the room, on the wall, hung a beautiful bear-skin rug.\\nThese rugs were for prizes, one for the girls and one for the boys. And this was the game.\\nThe girls were gathered at one end of the room and the boys at the other, and one end was called the North Pole, and the other the South Pole. Each player was given a small flag which they were to plant on reaching the Pole.\\nThis would have been an easy matter, but each traveller was obliged to wear snowshoes.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(df.loc[0, \"excerpt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T21:32:19.595971Z",
     "start_time": "2021-06-08T21:32:19.446095Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"word_count\"] = df['excerpt'].str.split().apply(len)\n",
    "df[\"word_count\"].hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T21:32:20.618366Z",
     "start_time": "2021-06-08T21:32:20.609636Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "https://www.africanstorybook.org/                                                                       118\n",
       "https://www.africanstorybook.org/#                                                                       46\n",
       "https://simple.wikipedia.org/wiki/Voltage                                                                 2\n",
       "https://emedia.uen.org/courses/utah-oer-textbooks-3rd-grade-seed/view                                     1\n",
       "https://www.commonlit.org/texts/who-gets-to-be-president                                                  1\n",
       "                                                                                                       ... \n",
       "https://freekidsbooks.org/wp-content/uploads/2019/09/Why-Cant-We-Glow-Like-Fireflies-FKB-Stories.pdf      1\n",
       "https://freekidsbooks.org/wp-content/uploads/2019/12/ELA_Grade2_Unit4_Workbook_engageNY-FKB.pdf           1\n",
       "https://en.wikipedia.org/wiki/Earthquake_valve                                                            1\n",
       "https://simple.wikipedia.org/wiki/Router                                                                  1\n",
       "https://en.wikipedia.org/wiki/Metabolism                                                                  1\n",
       "Name: url_legal, Length: 667, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"url_legal\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T21:58:38.967989Z",
     "start_time": "2021-06-08T21:58:38.935257Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/anjum/kaggle/commonlitreadabilityprize/\")\n",
    "\n",
    "from src.tokenizers import SentencePieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T21:34:14.562433Z",
     "start_time": "2021-06-08T21:34:12.243445Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ded4ef011e30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"google/bigbird-roberta-base\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_CACHE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# model = AutoModelForSequenceClassification.from_pretrained(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0mtokenizer_class_py\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTOKENIZER_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muse_fast\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1717\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1719\u001b[0;31m         return cls._from_pretrained(\n\u001b[0m\u001b[1;32m   1720\u001b[0m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_configuration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1789\u001b[0m         \u001b[0;31m# Instantiate tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1790\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1791\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1792\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1793\u001b[0m             raise OSError(\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.8/site-packages/transformers/models/big_bird/tokenization_big_bird_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, tokenizer_file, unk_token, bos_token, eos_token, pad_token, sep_token, mask_token, cls_token, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mmask_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAddedToken\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstrip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrstrip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmask_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mtokenizer_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mfast_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_slow_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslow_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    106\u001b[0m                 \u001b[0;34m\"Couldn't instantiate the backend tokenizer from one of: \\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0;34m\"(1) a `tokenizers` library serialization file, \\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one."
     ]
    }
   ],
   "source": [
    "model_name = \"google/bigbird-roberta-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=MODEL_CACHE)\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     model_name,\n",
    "#     cache_dir=MODEL_CACHE,\n",
    "# #     num_labels=1,\n",
    "# #     output_hidden_states=True,\n",
    "# )\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=MODEL_CACHE,\n",
    "#     num_labels=1,\n",
    "#     output_hidden_states=True,\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T20:43:32.184567Z",
     "start_time": "2021-06-03T20:43:32.125070Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\", return_token_type_ids=True)\n",
    "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T20:43:36.201070Z",
     "start_time": "2021-06-03T20:43:36.173700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0478,  0.0886, -0.0098,  ..., -0.0544, -0.0672, -0.0039],\n",
       "         [-0.0712,  0.0150, -0.1299,  ...,  0.0638,  0.0296, -0.0860],\n",
       "         [ 0.0906,  0.1437,  0.0828,  ...,  0.0509, -0.0320, -0.0490],\n",
       "         ...,\n",
       "         [ 0.0853,  0.2155,  0.0849,  ..., -0.1150,  0.0330, -0.0790],\n",
       "         [ 0.1679,  0.1288,  0.0065,  ...,  0.0367, -0.0631,  0.0276],\n",
       "         [-0.0436,  0.0892, -0.0389,  ..., -0.0957, -0.0744, -0.0284]]],\n",
       "       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[-3.2083e-03, -2.1940e-01, -2.1088e-01, -7.6721e-02,  1.2052e-01,\n",
       "          2.0488e-01,  2.6070e-01, -8.4343e-02, -7.2521e-02, -1.7025e-01,\n",
       "          2.1089e-01, -2.1008e-02, -8.2023e-02,  1.0177e-01, -1.4424e-01,\n",
       "          4.9201e-01,  2.1202e-01, -4.5743e-01,  3.5992e-02, -1.5410e-02,\n",
       "         -2.7218e-01,  8.2779e-02,  4.7006e-01,  3.3553e-01,  1.1576e-01,\n",
       "          6.0713e-02, -1.3375e-01, -1.2829e-02,  1.8398e-01,  2.2060e-01,\n",
       "          2.8606e-01,  6.4952e-02,  8.1128e-02,  2.3803e-01, -2.4037e-01,\n",
       "          6.3118e-02, -3.1076e-01,  2.3125e-02,  2.6080e-01, -1.8651e-01,\n",
       "         -7.9452e-02,  1.6443e-01,  2.1045e-01, -1.1840e-01, -1.1290e-01,\n",
       "          4.0572e-01,  2.5643e-01,  1.2402e-02, -1.4012e-01, -9.0373e-02,\n",
       "         -3.5431e-01,  3.3689e-01,  2.8484e-01,  1.9504e-01, -3.8899e-03,\n",
       "          5.9286e-02, -1.4538e-01,  2.5130e-01, -8.0821e-02, -9.2137e-02,\n",
       "         -1.1770e-01, -2.0288e-01, -1.3296e-02, -5.4601e-02,  2.9600e-02,\n",
       "         -1.4047e-01,  9.0024e-02, -1.4624e-01, -1.4190e-01,  5.4813e-02,\n",
       "         -8.5831e-02,  1.5300e-01,  1.6914e-01, -2.9596e-01, -2.9194e-01,\n",
       "          4.4366e-02, -5.8714e-01, -9.9223e-02,  2.9637e-01,  4.2659e-01,\n",
       "         -1.2049e-01,  1.8538e-01,  3.9120e-02,  2.1080e-01, -1.1054e-04,\n",
       "         -4.0333e-02, -3.0292e-02, -1.1374e-01,  1.9258e-01,  2.7085e-01,\n",
       "         -1.9872e-01, -3.7765e-01,  6.6629e-02,  1.1977e-02, -9.2962e-02,\n",
       "          2.1994e-02, -2.3806e-02, -9.6243e-02, -1.5413e-01, -1.6730e-01,\n",
       "          5.4596e-02, -2.6967e-01, -1.4077e-01,  2.6593e-01, -3.2843e-02,\n",
       "         -1.9712e-01, -7.7840e-03,  3.1161e-01,  7.4520e-02, -1.1691e-01,\n",
       "         -1.9002e-01,  4.2707e-01,  3.0891e-01,  1.9908e-04,  3.0902e-03,\n",
       "          1.7533e-01,  1.3256e-01, -2.9231e-01,  4.0937e-01, -3.1320e-01,\n",
       "         -1.7837e-02, -9.4238e-02,  1.1146e-01,  1.5994e-01, -2.2116e-01,\n",
       "          2.8568e-01,  1.3704e-01,  2.6840e-01,  1.8057e-01,  1.0697e-01,\n",
       "         -2.9501e-02,  1.4506e-01, -9.1473e-02,  1.3043e-01,  2.3266e-01,\n",
       "          1.1518e-01, -8.2936e-03, -3.2977e-01, -2.0575e-01,  2.6943e-01,\n",
       "          3.4101e-01,  1.5848e-01, -5.1063e-02,  1.8997e-01,  8.5725e-02,\n",
       "          2.2128e-01,  1.4372e-01, -4.1022e-01,  4.1918e-02,  3.5082e-01,\n",
       "          9.5941e-02,  1.5608e-01, -7.9715e-02, -2.8092e-01, -2.6411e-01,\n",
       "         -8.8306e-02,  5.0072e-02, -3.2875e-01, -1.3208e-01,  3.6529e-01,\n",
       "          2.1469e-02, -9.1301e-03, -1.4111e-01, -2.4637e-01, -2.3832e-02,\n",
       "         -1.2317e-01,  1.0649e-02,  1.1023e-01, -8.7270e-02, -4.0457e-01,\n",
       "         -8.6754e-02, -5.5748e-01, -1.0560e-01,  1.7113e-01, -3.2426e-01,\n",
       "          2.5368e-01, -2.7680e-01,  1.0262e-01,  4.0273e-01,  4.1042e-02,\n",
       "         -6.0418e-03, -1.8827e-01, -1.8964e-02,  9.2425e-02,  3.3184e-01,\n",
       "          2.4544e-01, -4.0065e-01,  1.1210e-01,  1.4380e-01,  2.5446e-01,\n",
       "          1.4272e-01, -5.5810e-02, -1.2864e-01,  1.5657e-01, -2.0099e-01,\n",
       "          1.7956e-01, -2.2109e-01,  1.8028e-01, -2.5524e-01, -2.1461e-01,\n",
       "          2.8902e-01, -3.9955e-01, -2.0351e-02,  8.7447e-02,  2.6895e-01,\n",
       "          1.2951e-02, -3.0363e-02, -7.7951e-02,  1.2212e-01,  1.9052e-01,\n",
       "          1.3431e-01, -3.8683e-01,  2.6582e-01, -2.7070e-02, -2.0792e-02,\n",
       "         -2.9300e-02,  1.6273e-01,  2.4574e-01,  9.6821e-02, -3.8570e-01,\n",
       "         -1.4093e-01,  1.1557e-01,  2.8804e-01, -2.2367e-01,  1.6416e-01,\n",
       "         -2.8154e-01, -3.9087e-01, -1.5021e-01,  2.1086e-01,  2.3105e-01,\n",
       "          1.6857e-01, -2.7287e-01,  1.6306e-01, -9.6196e-02, -4.2966e-01,\n",
       "         -3.6699e-01, -1.0738e-01,  2.5007e-01,  1.7285e-01,  1.8841e-01,\n",
       "          2.3728e-01,  3.9772e-02,  1.2035e-01,  1.4498e-01,  1.5580e-01,\n",
       "         -1.5020e-01,  1.9035e-01, -3.5627e-01, -5.8517e-02, -2.6624e-01,\n",
       "         -1.9203e-01, -1.9466e-01,  3.9838e-01, -2.3064e-01,  2.3329e-01,\n",
       "          3.8728e-01, -3.0407e-01, -1.1507e-01,  1.4859e-01,  9.1981e-02,\n",
       "          9.4698e-02, -1.1759e-01,  1.9530e-01,  1.4820e-01, -1.0942e-01,\n",
       "          2.5250e-01,  2.3121e-03,  2.5350e-01,  1.6860e-01,  8.7136e-02,\n",
       "          1.3513e-01,  1.2709e-01, -1.4862e-01,  6.0751e-02,  9.3345e-03,\n",
       "         -1.5235e-02, -2.3177e-01, -1.5835e-01,  2.3159e-01, -5.2097e-02,\n",
       "          2.3692e-02, -1.7081e-01, -1.1216e-01,  2.9000e-02,  4.0367e-01,\n",
       "         -3.5810e-01,  2.5118e-01,  7.6397e-02,  1.5772e-01, -2.4908e-01,\n",
       "         -2.2593e-01,  8.8132e-02,  1.7857e-01, -4.1115e-01,  1.0124e-02,\n",
       "          1.7034e-01,  9.3064e-02,  2.0605e-01,  2.6011e-01,  8.8563e-03,\n",
       "         -1.1599e-01,  4.9098e-01, -1.6107e-01, -1.0854e-01,  2.5702e-01,\n",
       "         -2.7082e-01, -2.7938e-01,  2.4896e-01, -2.8771e-02,  2.9932e-01,\n",
       "          1.2622e-01,  5.4038e-02,  7.4917e-02, -6.0018e-01,  6.1034e-02,\n",
       "         -4.5302e-01,  9.6109e-03,  2.2218e-02, -8.2516e-02, -1.9849e-01,\n",
       "          1.4898e-01,  2.9653e-01, -2.5667e-01, -2.8793e-02,  1.8863e-01,\n",
       "          6.9257e-02, -1.2353e-01,  4.7548e-01, -1.0399e-02,  2.1192e-01,\n",
       "         -5.5704e-02,  2.7205e-01, -2.1447e-01,  2.6845e-01, -2.7371e-01,\n",
       "         -7.9793e-02,  1.9678e-02,  7.9384e-02,  6.9016e-02, -6.2468e-02,\n",
       "         -3.3971e-01,  2.3516e-01, -7.9373e-03, -5.4151e-02, -3.6996e-02,\n",
       "          1.0210e-01,  8.3842e-04,  4.8332e-02,  5.8442e-02,  3.1268e-01,\n",
       "          2.2963e-01, -1.5988e-02, -3.7113e-01, -2.7166e-02, -8.3582e-02,\n",
       "          3.8697e-02,  4.8938e-02, -1.5783e-02,  4.3969e-01, -8.1168e-02,\n",
       "          3.5309e-03, -1.4284e-01,  2.6100e-01,  2.1125e-01,  1.2627e-01,\n",
       "          1.2150e-01,  5.6126e-02,  1.2790e-01, -5.3467e-02, -7.7731e-03,\n",
       "         -1.5671e-01, -2.3348e-01, -2.7533e-01,  2.0882e-01, -2.3905e-01,\n",
       "         -1.6444e-01,  1.6090e-01,  2.1307e-01, -1.5447e-01,  1.4451e-01,\n",
       "          3.0499e-01,  1.0366e-01, -1.4900e-01,  2.6629e-01, -1.0861e-01,\n",
       "          9.7824e-02,  3.1754e-01, -1.5510e-02,  1.7810e-01,  4.9086e-01,\n",
       "          2.1100e-01, -3.6921e-01, -3.9460e-02, -2.1250e-01, -2.9532e-03,\n",
       "          2.3747e-01, -1.4730e-01,  2.0255e-01,  3.8513e-01,  3.0637e-01,\n",
       "          4.5606e-01,  8.6698e-03, -1.3313e-01,  8.3101e-02,  2.1972e-01,\n",
       "          3.5209e-02, -1.5206e-01, -1.8271e-01,  2.5397e-01,  5.9857e-02,\n",
       "         -1.4193e-01, -3.8307e-02, -1.4553e-01,  4.7165e-02, -1.3325e-01,\n",
       "         -3.9433e-01,  4.2436e-02,  1.9429e-01, -4.8001e-01,  8.4262e-02,\n",
       "         -2.7683e-01,  3.6440e-02, -2.3495e-01,  2.1043e-01, -2.1483e-01,\n",
       "         -1.1695e-01,  3.9920e-01, -8.2238e-02,  5.3366e-02, -1.8199e-01,\n",
       "         -1.4275e-01,  2.0019e-02,  1.1507e-02, -3.6021e-02, -2.6190e-02,\n",
       "          3.3418e-01, -1.3056e-01,  3.2988e-02,  2.2499e-02,  2.0725e-01,\n",
       "         -3.8817e-02,  1.9868e-01,  2.0599e-02, -1.3215e-01, -3.7478e-01,\n",
       "          1.3769e-01, -1.9972e-01, -4.2155e-01, -3.7575e-01,  3.5446e-01,\n",
       "         -1.2160e-01, -2.4805e-01, -2.1167e-01, -2.5744e-01,  7.9576e-02,\n",
       "          1.7727e-01,  4.6106e-01, -3.8363e-01, -8.9918e-02,  4.6798e-01,\n",
       "         -6.3311e-02, -1.8311e-01,  2.9651e-01,  1.8038e-01, -3.3584e-01,\n",
       "          3.3358e-01,  2.5996e-01, -4.4390e-02,  2.1432e-02,  5.1450e-01,\n",
       "          1.2765e-01,  2.0913e-01, -2.3102e-01,  4.4982e-01, -2.2196e-01,\n",
       "          3.2071e-01, -1.4369e-01, -2.1302e-01, -2.2435e-01, -5.3515e-03,\n",
       "          3.3233e-01,  1.8082e-01, -4.2011e-01, -1.1286e-01,  4.2059e-02,\n",
       "          3.3800e-01, -3.8471e-01, -9.5194e-02,  1.4146e-02, -3.5604e-01,\n",
       "          1.0313e-01,  9.5098e-02,  2.2574e-01, -3.7865e-01,  3.6247e-03,\n",
       "          4.0502e-01, -3.0943e-01,  1.2550e-01,  3.0371e-01,  8.1005e-02,\n",
       "          3.3761e-01, -4.3828e-02, -3.5592e-03,  4.5577e-02, -2.2371e-01,\n",
       "         -3.7315e-02,  1.3860e-01,  5.5346e-01,  1.4818e-01, -3.8170e-01,\n",
       "          8.5319e-02,  2.4381e-01, -1.7700e-01,  3.0341e-01, -8.3246e-02,\n",
       "         -5.3749e-02,  2.6875e-01, -4.7404e-02,  1.4566e-01, -8.5108e-02,\n",
       "         -2.3477e-01, -3.0237e-01,  3.7206e-01, -2.1425e-01, -1.1400e-01,\n",
       "         -1.5839e-01, -1.1716e-01, -1.4141e-01,  3.6058e-02, -3.9509e-01,\n",
       "          3.3778e-01,  1.2326e-01, -2.1119e-01, -9.7114e-02, -9.1938e-02,\n",
       "         -1.6368e-01, -2.2534e-01, -2.6148e-01,  4.4172e-01, -1.5848e-01,\n",
       "         -4.5032e-01,  2.5770e-01,  2.4804e-02,  3.4088e-01,  3.5758e-02,\n",
       "          1.0179e-01, -4.3535e-02,  1.3006e-01,  1.0202e-01, -1.2319e-01,\n",
       "          2.6838e-01,  5.6350e-02, -5.5955e-01, -1.2166e-01, -2.0871e-01,\n",
       "          7.0198e-02,  1.9283e-01, -3.4027e-01,  1.8359e-02,  3.0427e-02,\n",
       "          1.4175e-01,  2.0520e-02, -1.1092e-01, -7.2368e-02,  4.0556e-01,\n",
       "          2.3859e-01,  2.8483e-01,  9.0411e-02,  2.4168e-01, -1.5583e-02,\n",
       "         -3.3556e-01,  3.5720e-02,  8.4716e-02, -1.9951e-01,  4.2450e-01,\n",
       "         -1.0325e-01, -3.9120e-01, -6.8124e-02,  3.9697e-01,  1.1035e-01,\n",
       "         -2.6056e-02, -4.7428e-02,  2.0151e-01,  1.6093e-01, -1.3766e-01,\n",
       "          1.9095e-01, -4.2122e-02, -1.4172e-01, -1.1627e-01,  8.0168e-02,\n",
       "         -2.1798e-01,  4.4219e-02, -1.4472e-01, -9.5638e-03, -2.0081e-01,\n",
       "          7.9597e-03, -1.9085e-01,  2.5598e-01, -3.3713e-01,  1.1407e-01,\n",
       "          7.0708e-02,  2.9457e-01, -3.4398e-01, -1.6568e-01, -6.8814e-02,\n",
       "          1.5692e-01,  2.6861e-01,  3.4846e-01,  2.1363e-02,  1.2183e-02,\n",
       "         -1.5440e-01, -2.5803e-01,  7.1844e-02, -1.9390e-01,  1.3984e-01,\n",
       "          7.0863e-02,  2.6211e-01, -3.0336e-01, -1.8847e-01,  2.2263e-01,\n",
       "         -1.0745e-01, -1.5428e-01,  4.0873e-01,  2.4101e-01,  2.1457e-01,\n",
       "          2.5258e-02,  2.4745e-01,  3.0405e-02, -1.7431e-01, -1.2609e-01,\n",
       "         -2.5028e-01,  7.9450e-02, -7.4164e-02, -5.3013e-02, -6.4149e-02,\n",
       "         -1.0449e-01, -2.0651e-01, -1.6621e-01,  1.3898e-01,  1.4613e-01,\n",
       "          3.6762e-02, -5.0540e-02, -2.2787e-02, -2.7661e-01,  2.9619e-01,\n",
       "          2.0211e-02,  7.4367e-02, -6.5166e-02,  2.9048e-02, -1.4591e-01,\n",
       "          2.4087e-01,  2.1074e-01,  9.0950e-02, -1.9921e-01, -4.8711e-02,\n",
       "         -2.8255e-01, -3.6132e-01,  6.1972e-02,  1.3865e-01,  1.0557e-01,\n",
       "         -6.8828e-02, -2.8253e-01, -1.0248e-02, -1.4579e-01,  1.7499e-01,\n",
       "          1.5391e-02, -1.5322e-01, -8.2757e-02, -5.3386e-02, -5.5117e-02,\n",
       "          7.9734e-02, -2.1771e-01, -1.8813e-01, -1.1873e-01, -6.5332e-02,\n",
       "         -7.3576e-02,  3.5305e-01, -4.6619e-02,  2.8207e-01, -1.4859e-01,\n",
       "          1.0735e-03, -1.6637e-01,  1.1365e-01, -6.2794e-02,  6.2111e-02,\n",
       "          2.7882e-01, -4.4071e-01, -1.6083e-01, -2.9695e-03, -2.1326e-01,\n",
       "         -1.6536e-01, -6.1523e-02, -3.7208e-02,  2.3069e-01, -3.4058e-01,\n",
       "          2.3236e-01, -1.2023e-01,  1.9255e-01, -7.2060e-02, -2.5519e-01,\n",
       "         -1.5923e-01,  1.6881e-02,  2.4711e-01, -3.4725e-01, -2.3262e-01,\n",
       "         -2.7701e-01, -1.0317e-01, -9.3272e-02, -2.6039e-01,  4.2547e-01,\n",
       "         -1.0798e-01, -7.4447e-02,  1.9067e-02,  4.4333e-01,  2.0667e-01,\n",
       "          1.4671e-01,  2.1272e-01, -1.5556e-02,  2.9830e-02,  1.1363e-01,\n",
       "         -4.5824e-01,  2.2959e-01, -2.3105e-01, -1.1901e-01, -2.5508e-03,\n",
       "          8.7938e-02, -2.8578e-02,  1.0000e-02, -1.3353e-01, -1.1202e-01,\n",
       "          2.1694e-01, -3.6830e-01, -3.1860e-02,  2.5587e-01,  1.5419e-01,\n",
       "         -2.5011e-01,  4.0122e-02,  1.1360e-01,  3.7682e-01,  1.0048e-01,\n",
       "         -2.2502e-01,  1.2753e-01, -3.4334e-01, -4.1958e-02, -1.9138e-01,\n",
       "         -2.9233e-01,  1.5540e-01, -6.1172e-02,  7.5229e-02, -8.8815e-02,\n",
       "         -2.9441e-01,  2.1590e-01, -5.9937e-02, -7.0518e-02,  4.1725e-01,\n",
       "          3.5135e-02, -1.3039e-01,  1.5322e-01,  6.3148e-03,  7.2842e-03,\n",
       "         -1.0152e-01,  2.6193e-01,  2.1660e-01, -2.7403e-01,  1.4459e-01,\n",
       "         -1.3454e-01, -3.1701e-02, -1.1210e-01]], grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs  #[\"logits\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T13:41:55.354075Z",
     "start_time": "2021-06-03T13:41:55.350770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 31414,     6,   127,  2335,    16, 11962,     2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T13:41:55.460069Z",
     "start_time": "2021-06-03T13:41:55.355197Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9d77483a2373>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m   1738\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0minner_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1739\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "outputs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T13:50:07.715312Z",
     "start_time": "2021-06-03T13:50:07.699739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.embeddings.word_embeddings.weight\n",
      "roberta.embeddings.position_embeddings.weight\n",
      "roberta.embeddings.token_type_embeddings.weight\n",
      "roberta.embeddings.LayerNorm.weight\n",
      "roberta.embeddings.LayerNorm.bias\n",
      "roberta.encoder.layer.0.attention.self.query.weight\n",
      "roberta.encoder.layer.0.attention.self.query.bias\n",
      "roberta.encoder.layer.0.attention.self.key.weight\n",
      "roberta.encoder.layer.0.attention.self.key.bias\n",
      "roberta.encoder.layer.0.attention.self.value.weight\n",
      "roberta.encoder.layer.0.attention.self.value.bias\n",
      "roberta.encoder.layer.0.attention.output.dense.weight\n",
      "roberta.encoder.layer.0.attention.output.dense.bias\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.0.intermediate.dense.weight\n",
      "roberta.encoder.layer.0.intermediate.dense.bias\n",
      "roberta.encoder.layer.0.output.dense.weight\n",
      "roberta.encoder.layer.0.output.dense.bias\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias\n",
      "roberta.encoder.layer.1.attention.self.query.weight\n",
      "roberta.encoder.layer.1.attention.self.query.bias\n",
      "roberta.encoder.layer.1.attention.self.key.weight\n",
      "roberta.encoder.layer.1.attention.self.key.bias\n",
      "roberta.encoder.layer.1.attention.self.value.weight\n",
      "roberta.encoder.layer.1.attention.self.value.bias\n",
      "roberta.encoder.layer.1.attention.output.dense.weight\n",
      "roberta.encoder.layer.1.attention.output.dense.bias\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.1.intermediate.dense.weight\n",
      "roberta.encoder.layer.1.intermediate.dense.bias\n",
      "roberta.encoder.layer.1.output.dense.weight\n",
      "roberta.encoder.layer.1.output.dense.bias\n",
      "roberta.encoder.layer.1.output.LayerNorm.weight\n",
      "roberta.encoder.layer.1.output.LayerNorm.bias\n",
      "roberta.encoder.layer.2.attention.self.query.weight\n",
      "roberta.encoder.layer.2.attention.self.query.bias\n",
      "roberta.encoder.layer.2.attention.self.key.weight\n",
      "roberta.encoder.layer.2.attention.self.key.bias\n",
      "roberta.encoder.layer.2.attention.self.value.weight\n",
      "roberta.encoder.layer.2.attention.self.value.bias\n",
      "roberta.encoder.layer.2.attention.output.dense.weight\n",
      "roberta.encoder.layer.2.attention.output.dense.bias\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.2.intermediate.dense.weight\n",
      "roberta.encoder.layer.2.intermediate.dense.bias\n",
      "roberta.encoder.layer.2.output.dense.weight\n",
      "roberta.encoder.layer.2.output.dense.bias\n",
      "roberta.encoder.layer.2.output.LayerNorm.weight\n",
      "roberta.encoder.layer.2.output.LayerNorm.bias\n",
      "roberta.encoder.layer.3.attention.self.query.weight\n",
      "roberta.encoder.layer.3.attention.self.query.bias\n",
      "roberta.encoder.layer.3.attention.self.key.weight\n",
      "roberta.encoder.layer.3.attention.self.key.bias\n",
      "roberta.encoder.layer.3.attention.self.value.weight\n",
      "roberta.encoder.layer.3.attention.self.value.bias\n",
      "roberta.encoder.layer.3.attention.output.dense.weight\n",
      "roberta.encoder.layer.3.attention.output.dense.bias\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.3.intermediate.dense.weight\n",
      "roberta.encoder.layer.3.intermediate.dense.bias\n",
      "roberta.encoder.layer.3.output.dense.weight\n",
      "roberta.encoder.layer.3.output.dense.bias\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.attention.self.query.weight\n",
      "roberta.encoder.layer.4.attention.self.query.bias\n",
      "roberta.encoder.layer.4.attention.self.key.weight\n",
      "roberta.encoder.layer.4.attention.self.key.bias\n",
      "roberta.encoder.layer.4.attention.self.value.weight\n",
      "roberta.encoder.layer.4.attention.self.value.bias\n",
      "roberta.encoder.layer.4.attention.output.dense.weight\n",
      "roberta.encoder.layer.4.attention.output.dense.bias\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.intermediate.dense.weight\n",
      "roberta.encoder.layer.4.intermediate.dense.bias\n",
      "roberta.encoder.layer.4.output.dense.weight\n",
      "roberta.encoder.layer.4.output.dense.bias\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.attention.self.query.weight\n",
      "roberta.encoder.layer.5.attention.self.query.bias\n",
      "roberta.encoder.layer.5.attention.self.key.weight\n",
      "roberta.encoder.layer.5.attention.self.key.bias\n",
      "roberta.encoder.layer.5.attention.self.value.weight\n",
      "roberta.encoder.layer.5.attention.self.value.bias\n",
      "roberta.encoder.layer.5.attention.output.dense.weight\n",
      "roberta.encoder.layer.5.attention.output.dense.bias\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.intermediate.dense.weight\n",
      "roberta.encoder.layer.5.intermediate.dense.bias\n",
      "roberta.encoder.layer.5.output.dense.weight\n",
      "roberta.encoder.layer.5.output.dense.bias\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.attention.self.query.weight\n",
      "roberta.encoder.layer.6.attention.self.query.bias\n",
      "roberta.encoder.layer.6.attention.self.key.weight\n",
      "roberta.encoder.layer.6.attention.self.key.bias\n",
      "roberta.encoder.layer.6.attention.self.value.weight\n",
      "roberta.encoder.layer.6.attention.self.value.bias\n",
      "roberta.encoder.layer.6.attention.output.dense.weight\n",
      "roberta.encoder.layer.6.attention.output.dense.bias\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.intermediate.dense.weight\n",
      "roberta.encoder.layer.6.intermediate.dense.bias\n",
      "roberta.encoder.layer.6.output.dense.weight\n",
      "roberta.encoder.layer.6.output.dense.bias\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.attention.self.query.weight\n",
      "roberta.encoder.layer.7.attention.self.query.bias\n",
      "roberta.encoder.layer.7.attention.self.key.weight\n",
      "roberta.encoder.layer.7.attention.self.key.bias\n",
      "roberta.encoder.layer.7.attention.self.value.weight\n",
      "roberta.encoder.layer.7.attention.self.value.bias\n",
      "roberta.encoder.layer.7.attention.output.dense.weight\n",
      "roberta.encoder.layer.7.attention.output.dense.bias\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.intermediate.dense.weight\n",
      "roberta.encoder.layer.7.intermediate.dense.bias\n",
      "roberta.encoder.layer.7.output.dense.weight\n",
      "roberta.encoder.layer.7.output.dense.bias\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.attention.self.query.weight\n",
      "roberta.encoder.layer.8.attention.self.query.bias\n",
      "roberta.encoder.layer.8.attention.self.key.weight\n",
      "roberta.encoder.layer.8.attention.self.key.bias\n",
      "roberta.encoder.layer.8.attention.self.value.weight\n",
      "roberta.encoder.layer.8.attention.self.value.bias\n",
      "roberta.encoder.layer.8.attention.output.dense.weight\n",
      "roberta.encoder.layer.8.attention.output.dense.bias\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.intermediate.dense.weight\n",
      "roberta.encoder.layer.8.intermediate.dense.bias\n",
      "roberta.encoder.layer.8.output.dense.weight\n",
      "roberta.encoder.layer.8.output.dense.bias\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.attention.self.query.weight\n",
      "roberta.encoder.layer.9.attention.self.query.bias\n",
      "roberta.encoder.layer.9.attention.self.key.weight\n",
      "roberta.encoder.layer.9.attention.self.key.bias\n",
      "roberta.encoder.layer.9.attention.self.value.weight\n",
      "roberta.encoder.layer.9.attention.self.value.bias\n",
      "roberta.encoder.layer.9.attention.output.dense.weight\n",
      "roberta.encoder.layer.9.attention.output.dense.bias\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.intermediate.dense.weight\n",
      "roberta.encoder.layer.9.intermediate.dense.bias\n",
      "roberta.encoder.layer.9.output.dense.weight\n",
      "roberta.encoder.layer.9.output.dense.bias\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.attention.self.query.weight\n",
      "roberta.encoder.layer.10.attention.self.query.bias\n",
      "roberta.encoder.layer.10.attention.self.key.weight\n",
      "roberta.encoder.layer.10.attention.self.key.bias\n",
      "roberta.encoder.layer.10.attention.self.value.weight\n",
      "roberta.encoder.layer.10.attention.self.value.bias\n",
      "roberta.encoder.layer.10.attention.output.dense.weight\n",
      "roberta.encoder.layer.10.attention.output.dense.bias\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.intermediate.dense.weight\n",
      "roberta.encoder.layer.10.intermediate.dense.bias\n",
      "roberta.encoder.layer.10.output.dense.weight\n",
      "roberta.encoder.layer.10.output.dense.bias\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.attention.self.query.weight\n",
      "roberta.encoder.layer.11.attention.self.query.bias\n",
      "roberta.encoder.layer.11.attention.self.key.weight\n",
      "roberta.encoder.layer.11.attention.self.key.bias\n",
      "roberta.encoder.layer.11.attention.self.value.weight\n",
      "roberta.encoder.layer.11.attention.self.value.bias\n",
      "roberta.encoder.layer.11.attention.output.dense.weight\n",
      "roberta.encoder.layer.11.attention.output.dense.bias\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.intermediate.dense.weight\n",
      "roberta.encoder.layer.11.intermediate.dense.bias\n",
      "roberta.encoder.layer.11.output.dense.weight\n",
      "roberta.encoder.layer.11.output.dense.bias\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias\n",
      "classifier.dense.weight\n",
      "classifier.dense.bias\n",
      "classifier.out_proj.weight\n",
      "classifier.out_proj.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T14:03:36.693775Z",
     "start_time": "2021-06-03T14:03:36.685729Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_children of RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T14:13:15.592314Z",
     "start_time": "2021-06-03T14:13:15.580444Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'generator' and 'generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-93ea1345a4bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroberta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'generator' and 'generator'"
     ]
    }
   ],
   "source": [
    "model.roberta.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T13:45:05.375668Z",
     "start_time": "2021-06-03T13:45:05.366242Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0021,  0.0051, -0.0325,  ..., -0.0270,  0.0057,  0.0049],\n",
       "         [-0.0094, -0.0157,  0.0309,  ..., -0.0360,  0.0347,  0.0074],\n",
       "         [-0.0193, -0.0328, -0.0298,  ...,  0.0261,  0.0075, -0.0067],\n",
       "         ...,\n",
       "         [-0.0226,  0.0168, -0.0352,  ..., -0.0125,  0.0174, -0.0036],\n",
       "         [-0.0140,  0.0131,  0.0080,  ..., -0.0112,  0.0454, -0.0104],\n",
       "         [ 0.0108,  0.0106, -0.0346,  ...,  0.0383,  0.0252,  0.0271]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0330,  0.0084,  0.0032,  ...,  0.0120,  0.0296, -0.0196],\n",
       "         [ 0.0172,  0.0159,  0.0358,  ..., -0.0038, -0.0167, -0.0239]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0.], requires_grad=True)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_params = [param for name, param in model.named_parameters() if name.startswith(\"classifier\")]\n",
    "clf_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('kaggle': conda)",
   "language": "python",
   "name": "python38164bitkaggleconda0013a04193b845a7a07c668fa6fedcae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
